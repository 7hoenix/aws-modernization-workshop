= Digital Modernization

== Container Orchestration

****
*Expected Outcome:*

* 200 level differentiation ECS/Fargate Vs EKS
* Deploy an application To ECS/Fargate and EKS and show the difference 300 level

*Lab Requirements:*
* an Amazon Elastic Container Service Cluster
* an Amazon Elastic Container Service for Kubernetes Cluster

*Average Lab Time:*
45-60 minutes
****

=== Introduction

In this module, we're going to deploy applications using http://aws.amazon.com/ecs/[Amazon Elastic Container Service (Amazon ECS)] to
orchestrate our containers on top of http://aws.amazon.com/fargate/[AWS Fargate] using 
http://aws.amazon.com/cloud9/[AWS Cloud9] to drive our development and deployment.

==== Getting Started with Amazon ECS using AWS Fargate

Before we get started, here are some definitions you need to understand in order to deploy your application when creating your first Amazon ECS cluster.

[options="header"]
|=======================
| Object | Cluster
| Cluster | Logical grouping of tasks and services. Infrastructure may be shared between tasks and services running on the same cluster.
| Task Definition | Blueprint for our application. Defines attributes such as CPU and memory requirements, networking configuration, and container definitions.
| Container Definition | Configuration for a container to run as part of our task. Defines attributes of the container including port mappings, resources requirements, environment variables, etc.
| Service | Maintains a specified number of running simultaneous instances of a task definition in an ECS cluster.
|=======================

You'll deploy a service via Amazon ECS using AWS Fargate as the launch type. The Fargate launch type allows you to run your containerized applications 
without the need to provision and manage the backend infrastructure. Amazon ECS also can launch tasks and services using the EC2 launch type which runs containerized 
applications on Amazon EC2 instances that you manage. Amazon ECS is the orchestration service responsible for running docker containers and AWS Fargate is the underlying 
compute platform where the containers will run.

===== Pushing the Petstore images to Amazon Elastic Container Registry (ECR)

Before we can deploy our Petstore application to AWS Fargate, we need to push our Petstore images to https://aws.amazon.com/ecr/[Amazon Elastic Container Registry (ECR)].
Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. 
Amazon ECR is integrated with Amazon Elastic Container Service (ECS), simplifying your development to production workflow.

1. Switch to the tab where you have your Cloud9 environment opened and change to this modules directory by running:
+
[source,shell]
----
cd ~/environment/aws-modernization-workshop/modules/containerize-application
----
+
2. Log into your Amazon ECR registry using the helper provided by the AWS CLI in the Cloud9 terminal:
+
[source,shell]
----
eval $(aws ecr get-login --no-include-email)
----
+
3. Use the AWS CLI to get information about the two Amazon ECR repositories that were created for you ahead of time. One repository will be for the Petstore PostgreSQL backend 
and the other will be for the Petstore web frontend.
+
[source,shell]
----
aws ecr describe-repositories --repository-name petstore_postgres
----
+
[.output]
....
{
    "repositories": [
        {
            "registryId": "876614805374", 
            "repositoryName": "petstore_postgres", 
            "repositoryArn": "arn:aws:ecr:us-west-2:123456789012:repository/petstore_postgres", 
            "createdAt": 1533757748.0, 
            "repositoryUri": "123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres"
        }
    ]
}
....
+
[source,shell]
----
aws ecr describe-repositories --repository-name petstore_frontend
----
+
[.output]
....
{
    "repositories": [
        {
            "registryId": "876614805374", 
            "repositoryName": "petstore_frontend", 
            "repositoryArn": "arn:aws:ecr:us-west-2:123456789012:repository/petstore_frontend", 
            "createdAt": 1533757751.0, 
            "repositoryUri": "123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_frontend"
        }
    ]
}
....
+
Note the contents of the repositoryUri for each repostiory.
+
4. Verify that your Docker images exist by running the docker images command.
+
[source,shell]
----
docker images
----
+
[.output]
....
REPOSITORY                          TAG                 IMAGE ID            CREATED             SIZE
containerize-application_petstore   latest              22f901adffb3        32 minutes ago      626MB
<none>                              <none>              9b961113e5bf        33 minutes ago      564MB
postgres                            9.6                 506063568b80        2 days ago          237MB
maven                               3.5-jdk-7           84a20ec9fab9        3 weeks ago         483MB
lambci/lambda                       nodejs4.3           6c30c5c1b1e0        6 weeks ago         969MB
lambci/lambda                       python2.7           377732dd7a1f        6 weeks ago         974MB
lambci/lambda                       python3.6           acf16b1d5297        6 weeks ago         1.1GB
lambci/lambda                       nodejs6.10          da301bf4fe34        6 weeks ago         1.02GB
jboss/wildfly                       11.0.0.Final        6926d48f2e5b        4 months ago        617MB
....
+
5. Tag the postgres image so you can push the image to the ECR repository. Note: You need to replace the value
from the repositoryURI for petstore_postgres previously.
+
[source,shell]
----
docker tag postgres:9.6 123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres:latest
----
+
6. Run the following command to push this image to the ECR repository. Note: You need to replace the value with
the tag you applied in the previous step.
+
[source,shell]
----
docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres:latest
----
+
You should see the Docker image being pushed:
+
[source,shell]
----
docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres:latest
----
+
[.output]
....
The push refers to repository [123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres]
7856d1f55b98: Pushed 
a125032aca95: Pushed 
fcfc309521a9: Pushed 
4c4e9f97ac56: Pushed 
109402c6a817: Pushed 
6663c6c0d308: Pushed 
ed4da41a79a9: Layer already exists 
7c050956ab95: Layer already exists 
c6fcee3b341c: Layer already exists 
998e6abcfae7: Layer already exists 
df9515382700: Layer already exists 
0fae9a7d0574: Layer already exists 
add4404d0b51: Layer already exists 
cdb3f9544e4c: Layer already exists 
latest: digest: sha256:ca39b6107978303706aac0f53120879afcd0d4b040ead7f19e8581b81c19ecea size: 3243
....
+
7. Now you need to do the same thing with the petstore frontend. Tag the containerize-application_petstore image so you can push the image to the ECR repository. Note: You need to replace the value from the repositoryURI for petstore_frontend previously.
+
[source,shell]
----
docker tag containerize-application_petstore:latest 123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_frontend:latest
----
+
8. Run the following command to push this image to the ECR repository. Note: You need to replace the value with the tag you applied in the previous step.
+
[source,shell]
----
docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_frontend:latest
----
+
You should see the Docker image being pushed:
+
[source,shell]
----
docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_frontend:latest
----
+
[.output]
....
The push refers to repository [123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_frontend]
d09698a5c7b9: Pushed 
8acb26508304: Pushed 
9743103e1954: Pushed 
cfbd33e75dbe: Pushed 
bdc2cd9ee81f: Pushed 
68f2b534f819: Layer already exists 
25392e8f9f5a: Layer already exists 
0c8237d7452a: Layer already exists 
d9e554ca876f: Layer already exists 
43e653f84b79: Layer already exists 
latest: digest: sha256:1752fbb6c9d826148ef790c5ac4f99fcc2b48a5744543ba4c58a7edf3f7d625e size: 2417
....

===== Amazon ECS Task definitions

https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html[Amazon ECS Task definitions] are required to run Docker containers in Amazon ECS. For our pestore application, we will review the key pieces of a task definition before we deploy it. 

1. Switch to the tab where you have your Cloud9 environment opened.
+
2. Open the *petstore-fargate-task-definition.json* file by double clicking the filename in the lefthand navigation in Cloud9.
+
3. The file has the following contents:
+
.petstore-fargate-task-definition.json
[source,json]
----
{
  "family": "petstore",
  "networkMode": "awsvpc",
  "containerDefinitions": [{
      "name": "postgres",
      "image": "<YourAccountID>.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres:latest",
      "cpu": 512,
      "memoryReservation": 1024,
      "environment": [{
          "name": "POSTGRES_DB",
          "value": "petstore"
        },
        {
          "name": "POSTGRES_USER",
          "value": "admin"
        },
        {
          "name": "POSTGRES_PASSWORD",
          "value": "password"
        }
      ],
      "portMappings": [{
        "containerPort": 5432
      }],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "petstore",
          "awslogs-region": "us-west-2",
          "awslogs-stream-prefix": "petstore/postgres"
        }
      }
    },
    {
      "name": "petstore",
      "image": "<YourAccountID>.dkr.ecr.us-west-2.amazonaws.com/petstore_frontend:latest",
      "cpu": 512,
      "memoryReservation": 1024,
      "environment": [
        {
          "name": "DB_HOST",
          "value": "127.0.0.1"
        },
        {
          "name": "DB_NAME",
          "value": "petstore"
        },
        {
          "name": "DB_PASS",
          "value": "password"
        },
        {
          "name": "DB_PORT",
          "value": "5432"
        },
        {
          "name": "DB_URL",
          "value": "jdbc:postgresql://127.0.0.1:5432/petstore?ApplicationName=applicationPetstore"
        },
        {
          "name": "DB_USER",
          "value": "admin"
        }
      ],
      "portMappings": [{
        "containerPort": 8080
      }],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "petstore",
          "awslogs-region": "us-west-2",
          "awslogs-stream-prefix": "petstore/frontend"
        }
      }
    }
  ],
  "executionRoleArn": "arn:aws:iam::<YourAccountID>:role/petstoreExecutionRole",
  "requiresCompatibilities": [
    "FARGATE"
  ],
  "cpu": "1 vcpu",
  "memory": "2 gb"
}
----
+
4. Replace the *<YourAccountID>* placeholders with your https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html[Account ID] and save the file.

5. Create a new task definition from the JSON file by running this command in your Cloud9 terminal:
+
[source,shell]
----
aws ecs register-task-definition --cli-input-json file://~/environment/aws-modernization-workshop/modules/container-orchestration/petstore-fargate-task-definition.json
----

===== Create the Petstore Service with Amazon ECS using Fargate

1. Go to the AWS Management Console, click Services then select *Elastic Container Service* under Compute.
+
2. On the left hand navigation ensure *Clusters* is selected and click *Create Cluster*.
+
3. On the *Select cluster template* screen select *Networking only* which should have a *Powered by AWS Fargate* label and click *Next step*.
+
4. Enter a Cluster name of *petstore-workshop* and leave the Create VPC box *unchecked* and click *Create*.
+
5. Once your cluster is created, view your cluster and the *Services* tab should be seleced. Click *Create*.
+
6. Select a *Launch type* of *FARGATE*
+
7. Select *petstore* and the latest *revision* for the *Task Definition*.
+
8. Enter *petstore* for the *Service name*.
+
9. Enter *1* into *Number of tasks* and click *Next step*.
+
10. Select the *petstore* VPC for *Cluster VPC*.
+
11. Select the two *Public* petstore Subnets for *Subnets*.
+
12. For *Security Groups* click *Edit* then click *Select an existing Security Group*. Select the *default* Security Group and click Save. You will notice that the default Security Group only allows port 8080 from PetStoreLbSecurityGroup to secure our petstore application.
+
13. Select *ENABLED* from *Auto-assign public IP*. This allows your tasks to retrieve the Docker image from Amazon ECR and stream logs to Amazon CloudWatch Logs.
+
14. Set the *Health check grace period* to *300*. Note: This is the period of time, in seconds, that the Amazon ECS service scheduler should ignore unhealthy 
Elastic Load Balancing target health checks after a task has first started. This is only valid if your service 
is configured to use a load balancer. If your service's tasks take a while to start and respond to health checks, 
you can specify a health check grace period of up to 7,200 seconds during which the ECS service scheduler ignores the health check status. 
This grace period can prevent the ECS service scheduler from marking tasks as unhealthy and stopping them before they have time to come up.
+
15. Under Load Balancing, select the Application Load Balancer and make sure *petstore-lb* is selected.
+
16. For *Container to load balance* select *petstore:8080:8080* and click *Add to load balancer*.
+
17. Type in 80 for the *Listener port* and ensure it is set to *create new*.
+
18. Ensure that *Target group name* is set to *create new* and the name should be *ecs-petsto-petstore.
+
19. *Uncheck* the box for *Enable service discovery integration* and click *Next step*.
+
20. The next page allows you to define an Auto Scaling policy. Leave this set to *Do not adjust the service's desired count* for now and click *Next step*.
+
21. Review your settings and click *Create Service*.
+
22. The service will now start your task. Click *View Service* and you will have to wait for your task to transition to *RUNNING*. Feel free to inspect the logs for your task while you wait.
+
23. Once the task is running, view the *Details* of the petstore *Service*. Under *Load Balancing* click the *ecs-petsto-petstore* Target Group link to view 
the Tasks being registerd into the Target Group under *Targets*.
+
24. Once your Registerd Target is registered as *Healthy* you can view the service through the Application Load Balancer.
+
25. Click on the *Load Balancing* link on the left hand navigation pane of the EC2 window to view your Application Load Balancer.
+
26. For the *petstore-lb* copy the *DNS name* and navigate to http://<InsertYourCopiedDNSName>/applcationPetstore to see the petstore application running. 


==== Getting Started with Amazon Elastic Container Service for Kubernetes (EKS)

Before we get started, here are some terms you need to understand in order to
deploy your application when creating your first Amazon EKS cluster.

[options="header"]
|=======================
| Object | Cluster
| Cluster | A group of EC2 instances that are all running `kubelet` which
connects into the master control plane.
| Deployment | Configuration file that declares how a container should be
deployed including how many `replicas` what the `port` mapping should be and how
it is `labeled`.
| Service | Configuration file that declares the ingress into the container,
these can be used to create Load Balancers automatically.
|=======================

===== Amazon EKS and Kubernetes Manifests

To use Amazon EKS or any Kubernetes cluster you must write a manifest or a
config file, these config files are used to declaritively document what is
needed to deploy an individual application. More information can be found
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/[here]

1. Switch to the tab where you have your Cloud9 environment opened.

2. Open the *petstore-eks-manifest.yaml* file by double clicking the filename
in the lefthand navigation in Cloud9.

3. The file has the following contents:
+
.petstore-eks-manifest.yaml
[source,yaml]
----
apiVersion: v1
kind: List
items:
- apiVersion: v1
  kind: Namespace
  metadata:
    name: petstore

- kind: PersistentVolume
  apiVersion: v1
  metadata:
    name: postgres-pv-volume
    namespace: petstore
    labels:
      type: local
  spec:
    storageClassName: gp2
    capacity:
      storage: 20Gi
    accessModes:
      - ReadWriteOnce
    hostPath:
      path: "/mnt/data"

- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: postgres-pv-claim
    namespace: petstore
  spec:
    storageClassName: gp2
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: 20Gi

- apiVersion: v1
  kind: Service
  metadata:
    name: postgres
    namespace: petstore
  spec:
    ports:
    - port: 5432
    selector:
      app: postgres
    clusterIP: None

- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: postgres
    namespace: petstore
  spec:
    selector:
      matchLabels:
        app: postgres
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          app: postgres
      spec:
        containers:
        - image: <YourAccountID>.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres:latest
          name: postgres
          env:
          - name: POSTGRES_PASSWORD
            value: password
          - name: POSTGRES_DB
            value: petstore
          - name: POSTGRES_USER
            value: admin
          ports:
          - containerPort: 5432
            name: postgres
          volumeMounts:
          - name: postgres-persistent-storage
            mountPath: /var/lib/postgresql/data
            subPath: petstore
        volumes:
        - name: postgres-persistent-storage
          persistentVolumeClaim:
            claimName: postgres-pv-claim

- apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    namespace: petstore
  spec:
    selector:
      app: frontend
    ports:
    - port: 80
      targetPort: http-server
      name: http
    - port: 9990
      targetPort: wildfly-cord
      name: wildfly-cord
    type: LoadBalancer

- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    name: frontend
    namespace: petstore
    labels:
      app: frontend
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: frontend
    template:
      metadata:
        labels:
          app: frontend
      spec:
        initContainers:
        - name: init-frontend
          image: <YourAccountID>.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres:latest
          command: ['sh', '-c',
                    'until pg_isready -h postgres.petstore.svc -p 5432;
                    do echo waiting for database; sleep 2; done;']
        containers:
        - name: frontend
          image: <YourAccountID>.dkr.ecr.us-west-2.amazonaws.com/petstore_frontend:latest
          ports:
          - name: http-server
            containerPort: 8080
          - name: wildfly-cord
            containerPort: 9990
          env:
          - name: DB_URL
            value: "jdbc:postgresql://postgres.petstore.svc:5432/petstore?ApplicationName=applicationPetstore"
          - name: DB_HOST
            value: postgres.petstore.svc
          - name: DB_PORT
            value: "5432"
          - name: DB_NAME
            value: petstore
          - name: DB_USER
            value: admin
          - name: DB_PASS
            value: password
----
+
4. Replace the *<YourAccountID>* placeholders with your https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html[Account ID] and save the file.

5. Apply your manifest by running this command in your Cloud9 terminal:
+
[source,shell]
----
kubectl apply -f petstore-eks-manifest.yaml
----
+
[.output]
....
namespace/petstore created
persistentvolume/postgres-pv-volume configured
persistentvolumeclaim/postgres-pv-claim created
service/postgres created
deployment.apps/postgres created
service/frontend created
deployment.apps/frontend created
....
+
6. As you can see above this manifest created and configured several components
   in your Kubernetes cluster, we created a *namespace*, *persistentvolume*,
   *persistentvolumeclaim*, 2 *services*, and 2 *deployments*.
+
[options="header"]
|=======================
| Primitive | Description
| *Namespace* | Namespaces are meant to be virtual clusters within a larger
pysical cluster.
| *PersistentValue* | Persistent Volume (PV) is a piece of storage that has been
provisioned by an administrator. _These are cluster wide resources._
| *PersistentVolumeClaim* | Persistent Volume Claim (PVC) is a request for storage
by a user.
| *Service* | Service is an abstraction which defines a logical set of Pods
and a policy by which to access them.
| *Deployment* | Deployment controller provides declarative updates for Pods and
ReplicaSets. 
|=======================
+
7. Now that the scheduler knows that you want to run this application it will
   find available *disk*, *cpu* and *memory* and will place the jobs. Let's
   watch as they get provisioned.
+
[source,shell]
----
kubectl get pods --namespace petstore --watch
----
+
[.output]
....
NAME                        READY     STATUS              RESTARTS   AGE
frontend-869db5db6b-ht4h8   0/1       Init:0/1            0          3m
frontend-869db5db6b-j5nfj   0/1       Init:0/1            0          3m
postgres-678864b7-vs5zj     0/1       ContainerCreating   5          3m
....
+
8. Once the *STATUS* changes to *Running* for all 3 of your containers we can
   then load the services and navigate to the exposed application.
+
[source,shell]
----
kubectl get services --namespace petstore
----
+
[.output]
....
NAME       TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)                                     AGE
frontend   LoadBalancer   10.100.20.251   ac7059d97a51611e88f630213e88d018-2093299179.us-west-2.elb.amazonaws.com   80:30327/TCP,443:32177/TCP,9990:30543/TCP   6m
postgres   ClusterIP      None            <none>                                                                    5432/TCP                                    6m
....
+
9. Here we can see that we're exposing the *frontend* using an ELB which is
   available at the *EXTERNAL-IP* field. Copy and paste this into a new browser
   tab.
